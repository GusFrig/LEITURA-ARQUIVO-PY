{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8XkTNQ7RI4E",
        "outputId": "8460a69a-17c6-47e6-c563-6fe4b048c93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando processamento do arquivo: /content/drive/MyDrive/py/ap2-capes-ufc-2021.csv...\n",
            "Total de trabalhos carregados: 1287\n",
            "--------------------------------------------------\n",
            "### TOP 10 PROGRAMAS ###\n",
            "1. ECONOMIA: 61\n",
            "2. EDUCAÇÃO: 60\n",
            "3. LETRAS: 58\n",
            "4. QUÍMICA: 42\n",
            "5. DIREITO: 39\n",
            "6. LINGÜÍSTICA: 33\n",
            "7. ADMINISTRAÇÃO E CONTROLADORIA: 32\n",
            "8. FÍSICA: 30\n",
            "9. PSICOLOGIA: 30\n",
            "10. ENGENHARIA DE TELEINFORMÁTICA: 30\n",
            "--------------------------------------------------\n",
            "### TOP 10 ORIENTADORES ###\n",
            "1. SERGIO AQUINO DE SOUZA: 10\n",
            "2. WAGNER BANDEIRA ANDRIOLA: 7\n",
            "3. MAGNO JOSE DUARTE CANDIDO: 6\n",
            "4. ALESSANDRA CARVALHO DE VASCONCELOS: 6\n",
            "5. JOAO MÁRIO SANTOS DE FRANCA: 6\n",
            "6. MARTA MARIA DE FRANCA FONTELES: 6\n",
            "7. JOSE GERARDO VASCONCELOS: 6\n",
            "8. BARTOLOMEU WARLENE SILVA DE SOUZA: 5\n",
            "9. FERNANDO LUIZ MARCELO ANTUNES: 5\n",
            "10. BRENO MAGALHAES FREITAS: 5\n",
            "--------------------------------------------------\n",
            "### TOP 10 ÁREAS (Grande Área -> Área) ###\n",
            "1. CIÊNCIAS SOCIAIS APLICADAS -> ECONOMIA: 68\n",
            "2. LINGÜÍSTICA, LETRAS E ARTES -> LETRAS: 68\n",
            "3. CIÊNCIAS DA SAÚDE -> MEDICINA: 65\n",
            "4. ENGENHARIAS -> ENGENHARIA ELÉTRICA: 62\n",
            "5. CIÊNCIAS HUMANAS -> EDUCAÇÃO: 60\n",
            "6. MULTIDISCIPLINAR -> INTERDISCIPLINAR: 51\n",
            "7. CIÊNCIAS EXATAS E DA TERRA -> FÍSICA: 43\n",
            "8. ENGENHARIAS -> ENGENHARIA CIVIL: 43\n",
            "9. CIÊNCIAS EXATAS E DA TERRA -> QUÍMICA: 42\n",
            "10. CIÊNCIAS SOCIAIS APLICADAS -> DIREITO: 39\n",
            "--------------------------------------------------\n",
            "### TOP 20 PALAVRAS EM TÍTULOS (Threading) ###\n",
            "#    PALAVRA              FREQ \n",
            "1    para                 248  \n",
            "2    analise              123  \n",
            "3    avaliacao            103  \n",
            "4    estudo               97   \n",
            "5    ceara                92   \n",
            "6    sobre                86   \n",
            "7    como                 80   \n",
            "8    ensino               79   \n",
            "9    brasil               66   \n",
            "10   entre                52   \n",
            "11   saude                48   \n",
            "12   efeito               46   \n",
            "13   educacao             46   \n",
            "14   desenvolvimento      44   \n",
            "15   fortaleza            43   \n",
            "16   pacientes            39   \n",
            "17   estado               38   \n",
            "18   efeitos              38   \n",
            "19   modelo               32   \n",
            "20   brasileiro           30   \n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import threading\n",
        "import unicodedata\n",
        "import string\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from math import ceil\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CLASSES E MODELAGEM\n",
        "# ==============================================================================\n",
        "class Trabalho: # -> Criação da classe trabalho, que é o coração do código\n",
        "    def __init__(self, nm_programa, nm_grande_area, nm_orientador, nm_area, nm_producao):\n",
        "        self.programa = nm_programa\n",
        "        self.grande_area = nm_grande_area\n",
        "        self.orientador = nm_orientador\n",
        "        self.area = nm_area\n",
        "        self.titulo = nm_producao\n",
        "        # -> Colunas relevantes ditas na folha de questão\n",
        "    def __repr__(self):\n",
        "        return f\"<Trabalho: {self.titulo[:20]}...>\"\n",
        "#  -> A classe cria objetos trabalho, asssim como expresso na folha de questões\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÕES PURAS E UTILITÁRIAS\n",
        "# ==============================================================================\n",
        "\n",
        "# Função que irá carregar o stopwords(palavras chaves a serem removidas ou alteradas)\n",
        "def carregar_stopwords(caminhos):\n",
        "    \"\"\"Carrega stopwords de múltiplos arquivos.\"\"\"\n",
        "    stops = set() # -> Criação de um conjunto para facilitar o armazenamento e uso de funções de agregação como \"IN\"\n",
        "\n",
        "    for caminho in caminhos: # -> Um loop para leitura dos arquivos stopwords caminhos -> arquivo(s) caminho -> Arquivo-fonte\n",
        "        if os.path.exists(caminho):\n",
        "            with open(caminho, 'r', encoding='utf-8') as f: # Abertura e leitura do arquivo, com f sendo a variável que pasará lendo o arquivo\n",
        "                stops.update(line.strip().lower() for line in f)\n",
        "                # Update no conjunto stops, adicionando palavras baseado em espaços entres as mesmas, deixando tudo em lower. para cada linha da variável \"f\" que\n",
        "                # passa lendo os stops\n",
        "        else:\n",
        "            print(f\"Aviso: Arquivo de stopwords '{caminho}' não encontrado.\")\n",
        "    return stops\n",
        "# Aqui é uma função que irá normalizar as palavras encontradas e armazenadas em uma lista anteriormente definida de palavras\n",
        "def normalizar_token(token):\n",
        "    \"\"\"\n",
        "    Aplica normalização: minúsculas, remove pontuação, remove acentos.\n",
        "\n",
        "    \"\"\"\n",
        "    # 1. Minúsculas\n",
        "    token = token.lower()\n",
        "\n",
        "    # 2. Remove pontuação\n",
        "    token = token.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 3. Remove acentos (Normalização Unicode)\n",
        "    nfkd_form = unicodedata.normalize('NFKD', token)\n",
        "    token = \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "    return token\n",
        "# Leitura do Arquivo com nossos dados, lendo o arquivo e categorizando as coluna chave que serão usadas em toda lógica de visualização das posteriores informações\n",
        "def ler_dataset(caminho_arquivo):\n",
        "    \"\"\"Lê o CSV e retorna lista de objetos Trabalho. \"\"\"\n",
        "    trabalhos = []\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            # O delimitador do arquivo enviado é ';'\n",
        "            leitor = csv.DictReader(f, delimiter=';')\n",
        "            for row in leitor:\n",
        "                # Verifica se as chaves existem para evitar KeyError em arquivos malformados\n",
        "                if 'NM_PROGRAMA' in row:\n",
        "                    trabalhos.append(Trabalho(\n",
        "                        nm_programa=row.get('NM_PROGRAMA', ''),\n",
        "                        nm_grande_area=row.get('NM_GRANDE_AREA_CONHECIMENTO', ''),\n",
        "                        nm_orientador=row.get('NM_ORIENTADOR', ''),\n",
        "                        nm_area=row.get('NM_AREA_CONHECIMENTO', ''),\n",
        "                        nm_producao=row.get('NM_PRODUCAO', '')\n",
        "                    ))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Erro: Arquivo {caminho_arquivo} não encontrado.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler dataset: {e}\")\n",
        "        sys.exit(1)\n",
        "    return trabalhos\n",
        "# A função que irá gerar os rankings propostos na atividade\n",
        "def gerar_ranking_metadados(lista_objetos, extrator_chave, top_n=10):\n",
        "    \"\"\"\n",
        "    Função genérica para rankings simples.\n",
        "\n",
        "    \"\"\"\n",
        "    # Programação Funcional: Map para extrair apenas o campo desejado\n",
        "    chaves = map(extrator_chave, lista_objetos)\n",
        "\n",
        "    # Agregação\n",
        "    contador = Counter(chaves)\n",
        "\n",
        "    # Retorna os Top N\n",
        "    return contador.most_common(top_n)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PROGRAMAÇÃO CONCORRENTE: CONTAGEM DE PALAVRAS\n",
        "# ==============================================================================\n",
        "# Criação da primeira classe relacionda ao uso de thread -> Essa classe definirá o processamento das atividades que serma feitas por cada thread\n",
        "class WordCounterWorker(threading.Thread):\n",
        "    \"\"\"Thread responsável por processar um bloco de trabalhos.\"\"\"\n",
        "\n",
        "    def __init__(self, trabalhos_chunk, stopwords):\n",
        "        super().__init__()\n",
        "        self.trabalhos = trabalhos_chunk # A thread receberá uma parte dos trabalhos\n",
        "        self.stopwords = stopwords # As palvaras a serem usadas na filtragem (Palavras para serem removidas da contagem)\n",
        "        self.resultado_parcial = {} # Um mini dicionário que irá armazenar momentaneamente a contagem de palavras por thread, antes de ser colocada em um cojunto global\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Executa o processamento do bloco.\"\"\"\n",
        "        local_counter = {}\n",
        "\n",
        "        for trabalho in self.trabalhos:\n",
        "            # Tokenização simples por espaço\n",
        "            palavras_cruas = trabalho.titulo.split()\n",
        "\n",
        "            for palavra in palavras_cruas:\n",
        "                # Normalização\n",
        "                token = normalizar_token(palavra) # Uso na função de normalização de palavras feita lá em cima\n",
        "\n",
        "                # Filtros\n",
        "                # Ignorar <= 3 chars e stopwords\n",
        "                if len(token) <= 3:\n",
        "                    continue\n",
        "                if token in self.stopwords:\n",
        "                    continue\n",
        "\n",
        "                # Contagem manual (proibido lib de contagem aqui)\n",
        "                if token in local_counter:\n",
        "                    local_counter[token] += 1\n",
        "                else:\n",
        "                    local_counter[token] = 1\n",
        "\n",
        "        self.resultado_parcial = local_counter # O contador local joga as informaçõe processadas no dicinonário temporário criado\n",
        "\n",
        "# O método que organiza os trabalhos para as threads criadas\n",
        "def executar_contagem_concorrente(trabalhos, stopwords, n_threads=4):\n",
        "    \"\"\"\n",
        "    Orquestrador das threads. Divide dados e agrega resultados.\n",
        "\n",
        "    \"\"\"\n",
        "    tamanho_bloco = ceil(len(trabalhos) / n_threads) # definição dos blocos de contagem para cada thread. Sempre arredondando para cima\n",
        "    threads = []\n",
        "\n",
        "    # 1. Divisão e Início das Threads\n",
        "    for i in range(n_threads):\n",
        "        inicio = i * tamanho_bloco\n",
        "        fim = inicio + tamanho_bloco\n",
        "        chunk = trabalhos[inicio:fim]\n",
        "        # Definição do tamanho da chunk\n",
        "\n",
        "        if not chunk: continue\n",
        "\n",
        "        worker = WordCounterWorker(chunk, stopwords) # criação dos trabalhadores com base na classe que indica suas tarefas. Recebe um pedaço do bloco de palavras e as stopw como arg\n",
        "        threads.append(worker) # Adiciona os workers no dicionário de threads\n",
        "        worker.start() # Começa o processo com os workers\n",
        "\n",
        "    # 2. Aguarda término (Join)\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    # 3. Agregação Global (Main Thread)\n",
        "    contagem_global = {}\n",
        "    for t in threads:\n",
        "        parcial = t.resultado_parcial\n",
        "        for palavra, qtd in parcial.items():\n",
        "            if palavra in contagem_global:\n",
        "                contagem_global[palavra] += qtd\n",
        "            else:\n",
        "                contagem_global[palavra] = qtd\n",
        "      # Contador global é criado, para receber as palavras que virão dos contadores parciais de cada thread\n",
        "      # Aqui, por meio de loops aninhados, percorremos as threads, e percorremos a quantidade de palavras que existem nos contadores parciais de cada thread\n",
        "      # Realiza a contagem das palavras de forma manual. Se a palavra está no cont global, adiciona ela novamente, caso não, adiciona ela\n",
        "\n",
        "    # 4. Ordenação Final para Ranking (Top 20)\n",
        "    ranking = sorted(contagem_global.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "    return ranking\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MAIN PIPELINE\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Parsing de Argumentos\n",
        "    parser = argparse.ArgumentParser(description=\"Pipeline AP2 CAPES\")\n",
        "    parser.add_argument('--dataset', type=str, default='/content/drive/MyDrive/py/ap2-capes-ufc-2021.csv', help='Caminho do CSV')\n",
        "    args, unknown = parser.parse_known_args() # Modified line\n",
        "\n",
        "    print(f\"Iniciando processamento do arquivo: {args.dataset}...\")\n",
        "\n",
        "    # 2. Carregar Stopwords (Assume arquivos na mesma pasta)\n",
        "    stopwords = carregar_stopwords(['/content/drive/MyDrive/py/stopwords_en.txt', '/content/drive/MyDrive/py/stopwords_pt.txt'])\n",
        "\n",
        "    # 3. Leitura do Dataset\n",
        "    lista_trabalhos = ler_dataset(args.dataset)\n",
        "    print(f\"Total de trabalhos carregados: {len(lista_trabalhos)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 4. Rankings Simples (Map/Filter/Reduce logic via Counter)\n",
        "\n",
        "    # Top 10 Programas\n",
        "    rank_programas = gerar_ranking_metadados(lista_trabalhos, lambda t: t.programa, top_n=10)\n",
        "    print(\"### TOP 10 PROGRAMAS ###\")\n",
        "    for i, (prog, qtd) in enumerate(rank_programas, 1):\n",
        "        print(f\"{i}. {prog}: {qtd}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Top 10 Orientadores\n",
        "    rank_orientadores = gerar_ranking_metadados(lista_trabalhos, lambda t: t.orientador, top_n=10)\n",
        "    print(\"### TOP 10 ORIENTADORES ###\")\n",
        "    for i, (ori, qtd) in enumerate(rank_orientadores, 1):\n",
        "        print(f\"{i}. {ori}: {qtd}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Top 10 Áreas (Grande Área -> Área)\n",
        "    rank_areas = gerar_ranking_metadados(\n",
        "        lista_trabalhos,\n",
        "        lambda t: f\"{t.grande_area} -> {t.area}\",\n",
        "        top_n=10\n",
        "    )\n",
        "    print(\"### TOP 10 ÁREAS (Grande Área -> Área) ###\")\n",
        "    for i, (area, qtd) in enumerate(rank_areas, 1):\n",
        "        print(f\"{i}. {area}: {qtd}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 5. Ranking de Palavras (Concorrente)\n",
        "    print(\"### TOP 20 PALAVRAS EM TÍTULOS (Threading) ###\")\n",
        "    rank_palavras = executar_contagem_concorrente(lista_trabalhos, stopwords, n_threads=4)\n",
        "\n",
        "    print(f\"{'#':<4} {'PALAVRA':<20} {'FREQ':<5}\")\n",
        "    for i, (palavra, freq) in enumerate(rank_palavras, 1):\n",
        "        print(f\"{i:<4} {palavra:<20} {freq:<5}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}